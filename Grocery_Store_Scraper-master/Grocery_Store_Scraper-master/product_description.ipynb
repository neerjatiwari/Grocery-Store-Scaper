{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome() # With Head\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless')\n",
    "# chrome_options.add_argument('--no-sandbox')\n",
    "# driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "links = open('links1.txt', 'r')\n",
    "url_list = links.readlines()\n",
    "# print(url_list)\n",
    "# exit()\n",
    "check_123 = []\n",
    "for url in url_list[15:17]:\n",
    "\n",
    "    driver.get(url)\n",
    "    # driver.get(url.replace('\\n', ''))\n",
    "    time.sleep(8)   # 2 Sec for ssh\n",
    "    while True:\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//button[@ng-click='vm.pagginator.showmorepage()']\").click()\n",
    "            time.sleep(2)\n",
    "            print(\"Clicked Successfully\")\n",
    "        except:\n",
    "            break\n",
    "    html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "    soup = bs(html, 'html.parser')\n",
    "    products = soup.findAll(\"div\", {\"qa\": \"product\"})\n",
    "    # print(len(products))\n",
    "    directory = 'All_Data\\\\' + url.replace('https://www.bigbasket.com/pc/', '').replace('/?nc=bt\\n', '').replace('/?nc=nb\\n', '').replace('/?nc=cs\\n', '').replace('/', '\\\\')\n",
    "    # print(directory)\n",
    "    try:\n",
    "        os.makedirs(directory+'\\\\images\\\\large')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(directory + '\\\\images\\\\small')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    # d_dir = os.path.join(directory, 'data')     # Data Directory\n",
    "    data = open(os.path.join(directory, 'data.txt'), \"w\")\n",
    "    data.write(\"[\")\n",
    "    # print(directory)\n",
    "    for product in products:\n",
    "        # break\n",
    "        img = product.find(\"img\")['src']\n",
    "        image_small = img.replace('/media/uploads/p/mm/', '/media/uploads/p/s/')\n",
    "        image_large = img.replace('/media/uploads/p/s/', '/media/uploads/p/l/').replace('/media/uploads/p/mm/', '/media/uploads/p/l/')\n",
    "        del img\n",
    "\n",
    "        Brand = product.find(\"div\", {\"qa\": \"product_name\"}).find(\"h6\").text\n",
    "        Product = product.find(\"div\", {\"qa\": \"product_name\"}).find(\"a\").text\n",
    "        Quantity = product.find(\"span\", {\"data-bind\": \"label\"}).text\n",
    "        Price = product.find(\"span\", {\"class\": \"discnt-price\"}).text\n",
    "        hit_list = [Brand, Product, Quantity, Price]\n",
    "        check_123.append(hit_list)\n",
    "#         df=pd.DataFrame(hit_list)\n",
    "#         df.columns =['Product']\n",
    "#         df.index = ['Brand', 'Product', 'Quantity', 'Price']\n",
    "#         df_1 = df.T\n",
    "\n",
    "        # Writing data jo json file\n",
    "        data.write(json.dumps({\n",
    "            'image': [{'small': image_small}, {'large': image_large}],\n",
    "            'Brand': Brand,\n",
    "            'Product': Product,\n",
    "            'Quantity': Quantity,\n",
    "            'Price': Price\n",
    "        })+',\\n\\n')\n",
    "\n",
    "        # copy image files\n",
    "        try:\n",
    "            image_small_raw = requests.get(image_small, stream=True)\n",
    "            filename = os.path.basename(image_small)\n",
    "            img_dir = os.path.join(directory, \"images\\\\small\", filename)\n",
    "            with open(img_dir, \"wb\") as out_file:\n",
    "                shutil.copyfileobj(image_small_raw.raw, out_file)\n",
    "            del image_small_raw\n",
    "\n",
    "            image_large_raw = requests.get(image_large, stream=True)\n",
    "            time.sleep(1)\n",
    "            #print(type(image_small_raw.raw))\n",
    "            filename = os.path.basename(image_large)\n",
    "            img_dir = os.path.join(directory, \"images\\\\large\", filename)\n",
    "            with open(img_dir, \"wb\") as out_file:\n",
    "                shutil.copyfileobj(image_large_raw.raw, out_file)\n",
    "            del image_large_raw\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        else:\n",
    "            pass \n",
    "            #print(\"Downloaded\", url)\n",
    "#         from tabulate import tabulate\n",
    "#         pretty_link = tabulate(df_1, headers=['Category', 'Brand','Product', 'Quantity', 'Price'])\n",
    "#         print(pretty_link)\n",
    "    data.write(\"]\")\n",
    "    data.close()\n",
    "df=pd.DataFrame(check_123, columns= ['Brand', 'Product', 'Quantity','Price'])\n",
    "# df.columns =['Product']\n",
    "# df.index = ['Brand', 'Product', 'Quantity', 'Price']\n",
    "#print(df)\n",
    "driver.quit()\n",
    "links.close()\n",
    "df\n",
    "#df.to_csv('bigbasket_product_data_tabular.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
